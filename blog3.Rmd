---
title: "Analysis of Ours to Shape Comments"
subtitle: "Blog Post, the third"
author: "Michele Claibourn"
date: "December 15, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(quanteda) # main text package
library(tidyverse) # for dplyr, stringr, piping, etc.
library(RColorBrewer) # better colors in graphs
library(scales) # better breaks and labels in graphs
```

## Introduction
To recap, we're exploring the comments submitted to President Ryan's Ours to Shape website (as of December 7, 2018). 

In the first post we looked at the distribution of comments across Ryan's three categories -- community, discovery, and service -- and across the contributors' primary connection to the university. We extracted features like length and readability of the comments, and compared these across groups. And we explored the context in which key words of interest (to me) were used.

In the second post we examined word frequencies, relative word frequencies by groups, and began identifying key words that were differentially associated with comment categories or contributor connections via comparison clouds, keyness, and term frequency-inverse document frequency. 

In this third post, we're going to step back and look for word collocations to uncover important multi-word phrases, or n-grams, that we might want to retain in our document-feature matrix. We'll also look at feature co-occurrence within groups to better understand how words are used together. And we'll look at document similarity, in part to find those comments that were submitted multiple times by the same person (sneaky!).

```{r data}
setwd("~/Box Sync/mpc/textanalysis2018_final/") 
load("results2.RData")
```

## N-Grams

We'll start with the initial corpus we created and tokenize it -- break it up into a bag of words. While we're at it, we'll remove punctuation, because I know I'm not interested in punctuation at the moment (though there are questions for which punctuation might be useful). Plus, I'll remove stop words, but retain the space those words occupied so that words that aren't originally adjacent don't become adjacent as a result. I make everything lower case (not always useful for bigrams when one is seeking proper names). Then we'll create each bigram, every two-word sequence that exists in the corpus, and estimate the bigrams that occur more frequently than is probable given the frequency of each of the individual word's presence in the corpus.

```{r ngram}
# collocation analysis, finding multi-word expressions, ngrams/phrases
comments_tokens <- tokens(comments_corpus, remove_punct = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_remove(stopwords("en"), padding = TRUE)

comments_col <- comments_tokens %>% 
  textstat_collocations(min_count = 10)
head(comments_col, 40)
```
These are the top 40 bigrams -- you can see how frequently each occurs, along with an estimated rate of occurrence ($\lambda$), and a Wald test Z-statistic testing the significance of the rate parameter. 

Many of these should be recognizable to UVA aficianados -- first year, President Ryan, Alderman Library -- or even those vaguely attentive to the larger discourse -- climate change, global warming, ice arena... wait, what!? Ice skating, ice sports, ice arena, ice rink, figure skating -- the closure of the downtown ice skating rink last April has clearly been on some contributors' minds.

And no Thomas Jefferson? There are some regularities I've come to expect when folks are discussing UVA, but good on you for not making it all about TJ (no worries, he's there, but is only the 69th most significant bigram, with only 13 occurrences).

Let's keep some of these multiword expressions together: climate change, global warming, health care, President Ryan, Alderman Library, and free speech to start. Here are the first 5 occurrences of "President Ryan" to give a sense of what we've changed:

```{r ngramretain}
# retain selected multi-word expressions
comments_comptokens <- tokens_compound(comments_tokens, comments_col[c(1,5,8,18,19,34)])
# kwic can be called on token objects as well
head(kwic(comments_comptokens, c('president_ryan')), 5)
```

The above is showing the keyword in context, but since we've removed stop words and punctuation, we're seeing only what remains -- not sentence fragments, but strings of words, minus stop words, surrounding the selected bigram.

## Feature Co-Occurrence

Another way we might understand what contributors to Ours to Shape are saying is through feature co-occurrence. Feature (or term, or word) co-occurrence within documents (or some smaller unit) let's us explore the relationsihp between the words used in the comments. Specifically, we can populate a square matrix, with rows and columns defined by the features, that counts the number of times any two words appear together in a document. This matrix can then be graphed as a network. 

For example, here I've trimmed the document feature matrix -- removing words that occur fewer than five times throughout the corpus -- to reduce the dimensionality further, created a feature co-occurrence matrix (fcm), and selected just the 20 most frequent words. The first six rows/columns of the resulting matrix are:
 
```{r fcm1}
# a. ours to shape comments
comments_dfm_comptokens <- dfm(comments_comptokens, remove = "") # create dfm, remove padding from tokens object
comments_dfm_trim <- dfm_trim(comments_dfm_comptokens, min_termfreq=5) # reduce dfm, remove features occurring fewer than 5 times
comments_fcm <- fcm(comments_dfm_trim) # generate feature co-occurrence matrix
comment_top <- names(topfeatures(comments_dfm_trim, 20)) # list of 20 most frequently words in dfm
comments_fcm <- fcm_select(comments_fcm, comment_top) # select these 20 features from fcm
head(comments_fcm)
```

This tells us that the word "students" co-occurs with itself 759 times, and co-occurs with the term "UVA" 1511 times; and the words "community" and "grounds" co-occur 249 times. The zeroes below the diagonal don't mean anything; the matrix is symmetric so removing those counts just makes it easier to read.

Let's visualize this in a network!

```{r fcm1fig, fig.height = 5, fig.width = 6, fig.align = "center"}
# visualize semantic network
size <- log(colSums(dfm_select(comments_dfm_trim, comment_top))) # scale for vertices
textplot_network(comments_fcm, min_freq = 0.5, omit_isolated = FALSE, 
                 vertex_size = size / max(size) * 3)
ggsave("blog3_1.jpg", width = 6, height = 5)
```

The figure shows the top 20 words; the nodes (the dots) are sized to reflect overall frequency (the log of the frequency in the dfm, to be precise), but as these are all highly-occuring words, the differences aren't very noticeable.

The edges, or the lines connecting the nodes, convey how frequently the connected terms appear together (in the same comment) -- heavier lines denote more frequent co-occurrence.

So while the word "university" co-occurs with almost everything, the word "service" co-occurs primarily with "community", "uva", and "students", among the top 20 words (there are other words with which "service" will co-occur more frequently among words not listed here).

Let's focus this more substantively on words that co-occur with "Charlottesville". I've extracted up to 10 words on either side of each appearance of "Charlottesville" (I used the key word in context function from the first blog post, but searched the tokenized object with stopwords removed) and constructed a feature co-occurrence matrix of these words, then selected the 50 most frequenty words from this truncted dfm.

```{r fcm2}
# focused on a keyword: charlottesville
kwic_cville <- kwic(comments_comptokens, "charlottesville", 10) # 10 word window
kwic_cville_dfm <- dfm(paste(kwic_cville$pre, kwic_cville$keyword, kwic_cville$post, sep = " "))
comments_fcm_cville <- fcm(kwic_cville_dfm) # generate feature co-occurrence
comment_top_cville <- names(topfeatures(kwic_cville_dfm, 50)) # list of most frequent words
comments_fcm_cville <- fcm_select(comments_fcm_cville, comment_top_cville) # select these features from fcm
head(comments_fcm_cville)
```

Here we see the first six rows/columns of the matrix (they aren't in any meaningful order). And below we turn this into a network, where the size of the nodes reflects the overall frequency of the term in the dfm and the width of the line reflects how frequently the two connected words occur together in the extracted window of words around "Charlottesville".

```{r fcm2fig, fig.height = 5, fig.width = 6, fig.align = "center"}
# visualize semantic network
size <- log(colSums(dfm_select(kwic_cville_dfm, comment_top_cville))) # scale for vertices
set.seed(11)
textplot_network(comments_fcm_cville, min_freq = 0.5, 
                 vertex_size = size / max(size) * 3)
ggsave("blog3_2.jpg", width = 6, height = 5)
```

In short, this shows the words that appear in close proximity to Charlottesville (within 10 words) and the strength of the relationship between the words as a function of their being used in the same window.

Charlottesville occurs most frequently in this set -- the set was selected to only include windows of words surrounding Charlottesville, so that's what we'd expect. But the word "community" has stronger connections (co-occurrences) with a wider range of words. You can see a few clusters of connected words: deadline, heat, days, century, which looks like the comments around global warming perhaps; and Virginia, region, beyond, communities, which may be about getting off grounds and into the world. 

## Document Similarity

In exploring the corpus, I came across what appeared to be duplicate comments. I want to dig into that a little bit, so we'll turn to document similarity. 

We'll use cosine similarity to measure the similarity between each pair of comments. This measures the cosine of the angle between two documents, or between the vectors of word counts that we're using to represent the documents. 

The result will be an $n \times n$ matrix of similarity metrics. Here are the cosine similarities for the first five documents:

```{r simil}
# similarity across all comments, save as matrix
comments_simil <- textstat_simil(comments_dfm_comptokens,  # create similarity matrix
                                 selection = docnames(comments_dfm_comptokens), 
                                 method = "cosine", margin = "documents",
                                 upper = TRUE, diag = TRUE) %>% 
  as.matrix  # format as matrix
comments_simil[1:5, 1:5]
diag(comments_simil) <- 0  # set diagonal to 0 (self-similarity)
```

Values of 1 represent identical documents -- none of the first five comments in the corpus are especially similar. Filtering the matrix for rows with cosine similarities of 0.99 or greater produces 56 pairs, representing 12 distinct comments each repeated multiple times.

```{r dups}
dups <- as.data.frame(which(comments_simil > 0.99, arr.ind = T, useNames = T)) # extract similar pairs
head(dups, 5)
dups_remove <- c("556", "538", "554", "731", "540", "728")
comments2 <- comments %>% filter(!(doc_id %in% dups_remove))
```

That is, comment 153 is a duplicate or near duplicate of comments 200, 210, 538, 554, and 731. I looked at the duplicate comments and it turns out most are submitted by different people -- some organized commenting. Most of the coordinated comments note the need for a new ice skating rink -- the following comments appeared 6 and 5 times, respectively.

```{r ice}
comments$text[153] # 6
comments$text[155] # 5
```

A few of these, though, represented multiple submissions by the same contributor, but under different comment categories -- once under community and once under service, for example. I removed six such duplicates, so the next post will use this slightly reduced set.

```{r save}
save.image("results3.RData")
```

## Coming Up

This post has looked at three somewhat disconnected methods -- collocation analysis for discovery of key phrases as a pre-analysis step, feature co-occurrence and representation of semantic networks, and document similarity for uncovering near duplicates. Most of this is set up for some additional methods yet to come. Similarity and distance between term vectors will come up again when we look at clustering; feature co-occurrence will be key in topic modeling. 

But first, we'll take a short detour into sentiment analysis.